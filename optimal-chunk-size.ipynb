{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tiktoken\n",
    "import math\n",
    "from tokenizers import Tokenizer\n",
    "from PyPDF2 import PdfReader\n",
    "import tqdm as notebook_tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text: Attention Is All You Need\n",
      "Ashish Vaswani\u0003\n",
      "Google Brain\n",
      "avaswani@google.comNoam Shazeer\u0003\n",
      "Google Brain...\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the \"docs\" folder (adjust if needed)\n",
    "docs_folder = \"docs\"\n",
    "\n",
    "# Get all files in the folder\n",
    "files = os.listdir(docs_folder)\n",
    "\n",
    "# Look for a PDF file\n",
    "for filename in files:\n",
    "  if filename.endswith(\".pdf\"):\n",
    "    # Construct the full path to the PDF\n",
    "    pdf_path = os.path.join(docs_folder, filename)\n",
    "\n",
    "    # Open the PDF with PyPDF2\n",
    "    with open(pdf_path, 'rb') as pdf_file:\n",
    "      pdf_reader = PdfReader(pdf_file)\n",
    "\n",
    "      # Extract all text content\n",
    "      text = \"\"\n",
    "      for page in pdf_reader.pages:\n",
    "        text += page.extract_text()\n",
    "\n",
    "      # Store text content in the doc variable\n",
    "      doc = text\n",
    "      break  # Exit the loop after finding the first PDF\n",
    "\n",
    "# Handle the case where no PDF is found\n",
    "else:\n",
    "  print(\"No PDF file found in the 'docs' folder.\")\n",
    "\n",
    "# Now you can use the 'doc' variable containing the extracted text\n",
    "print(f\"Extracted text: {doc[:100]}...\")  # Print first 100 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tokenizers.Tokenizer object at 0x107749030>\n"
     ]
    }
   ],
   "source": [
    "# load model and tokenizer\n",
    "tokenizer = Tokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model_name = \"bert-base-uncased\"\n",
    "#model_name = \"cl100k_base\"\n",
    "\n",
    "#encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "#print(encoding)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_chunker(document, max_chunk_size, model):\n",
    "\n",
    "    tokenizer = Tokenizer.from_pretrained(model)\n",
    "    #tokenizer = tiktoken.encoding_for_model(model)\n",
    "    document_tokens = tokenizer.encode(document)\n",
    "    document_size = len(document_tokens)\n",
    "    # total chunk number\n",
    "    K = math.ceil(document_size / max_chunk_size)\n",
    "    # average integer chunk size\n",
    "    average_chunk_size = math.ceil(document_size / K)\n",
    "    # number of chunks with average_chunk_size - 1 \n",
    "    shorter_chunk_number = K * average_chunk_size - document_size\n",
    "    # number of chunks with average_chunk_size\n",
    "    standard_chunk_number = K - shorter_chunk_number\n",
    "\n",
    "    chunks = []\n",
    "    chunk_start = 0\n",
    "    for i in range(0, K):\n",
    "        if i < standard_chunk_number:\n",
    "            chunk_end = chunk_start + average_chunk_size\n",
    "        else:\n",
    "            chunk_end = chunk_start + average_chunk_size - 1\n",
    "        chunk = document_tokens[chunk_start:chunk_end]\n",
    "        chunks.append(tokenizer.decode(chunk))\n",
    "        chunk_start = chunk_end\n",
    "\n",
    "    assert chunk_start == document_size\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument 'identifier': 'Tokenizer' object cannot be converted to 'PyString'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[43mauto_chunker\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(chunks)\n",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m, in \u001b[0;36mauto_chunker\u001b[0;34m(document, max_chunk_size, model)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mauto_chunker\u001b[39m(document, max_chunk_size, model):\n\u001b[0;32m----> 3\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m#tokenizer = tiktoken.encoding_for_model(model)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     document_tokens \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(document)\n",
      "\u001b[0;31mTypeError\u001b[0m: argument 'identifier': 'Tokenizer' object cannot be converted to 'PyString'"
     ]
    }
   ],
   "source": [
    "chunks = auto_chunker(doc, 1024, tokenizer)\n",
    "print(chunks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "long-doc-chunking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
